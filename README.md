# auto-compleated-by-LSTM
simple autocomplete by using tokenize word and using LSTM for predicting next words

##LSTM


##Trie model

##Ngram

# ngram

In this module we build the n-gram Language Model. In the process, we learn a lot of the basics of machine learning (training, evaluation, data splits, hyperparameters, overfitting) and the basics of autoregressive language modeling (tokenization, next token prediction, perplexity, sampling). GPT is "just" a very large n-gram model, too. The only difference is that GPT uses a neural network to calculate the probability of the next token, while n-gram uses a simple count-based approach.

#data
Dataset has 5772 words, dataset is taken from twitter posts and comments (now x) expressing different shades of emotions.
Link of dataset [link](https://drive.google.com/file/d/132dnmXzKV9TaURcP0B22Q11i1pKX9L9q/view?usp=drive_link)

