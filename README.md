# auto-compleated-by-LSTM
simple autocomplete by using tokenize word and using LSTM for predicting next words

##LSTM
March 2022) Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models, and other sequence learning methods.

##Trie model

## Ngram

In this module we build the n-gram Language Model. In the process, we learn a lot of the basics of machine learning (training, evaluation, data splits, hyperparameters, overfitting) and the basics of autoregressive language modeling (tokenization, next token prediction, perplexity, sampling). GPT is "just" a very large n-gram model, too. The only difference is that GPT uses a neural network to calculate the probability of the next token, while n-gram uses a simple count-based approach.

#data
Dataset has 5772 words, dataset is taken from twitter posts and comments (now x) expressing different shades of emotions.

Link of dataset [link](https://drive.google.com/file/d/132dnmXzKV9TaURcP0B22Q11i1pKX9L9q/view?usp=drive_link)

